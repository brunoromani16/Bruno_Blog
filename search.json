[
  {
    "objectID": "posts/Final_Project.html",
    "href": "posts/Final_Project.html",
    "title": "First File",
    "section": "",
    "text": "https://www.science.org/doi/10.1126/science.aax2342\nThe article “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations” relased by Science examines how an algorithm used in healthcare can perpetuate racial bias. The article focuses on an algorithm that is used to identify patients who may benefit from additional healthcare services or lower costs. The author dives deep into one specific algorithm used in the industry, which he found in his study to exhibit racial bias. It presented racial bias by assigning a lower risk score to White patients over Black patients, even though they had similar similar scores throughout the different variables the algorithm would use to determine health risk score.\nA description to the dataset and how it was obtained can be found under this link: https://gitlab.com/labsysmed/dissecting-bias\nSome of the reasons as to why I chose this dataset to analyze include having a deeper understanding of algorithms that are racially biased, and determining if this algorithm in specific is racially biased. As a first generation student and person of color, taking a closer look at the data and how this algorithm works will help me understand how algorithms can be not only racially biased, but could lead inaccurate decisions and outcomes that may lead to people of color have inmense disavantages in several aspects of life, not only the health care system.\nTherefore, with this data set, I will take a high level overview of analysis to explore how risk scores are being assigned to patients. Specifically, I will try to answer the question what factors could make this algorithm racially bias? How is it affecting patients of color?\n\n\n\n\n\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n\nhealth_risk = pd.read_csv('data_new.csv')\nhealth_risk\n\n# we can see that there are variables that have NaN values, such as bps_mean_t or idl_mean_t\n# we will remove these in our analysis \n\n\n\n\n\n  \n    \n      \n      risk_score_t\n      program_enrolled_t\n      cost_t\n      cost_avoidable_t\n      bps_mean_t\n      ghba1c_mean_t\n      hct_mean_t\n      cre_mean_t\n      ldl_mean_t\n      race\n      ...\n      trig_min-high_tm1\n      trig_min-normal_tm1\n      trig_mean-low_tm1\n      trig_mean-high_tm1\n      trig_mean-normal_tm1\n      trig_max-low_tm1\n      trig_max-high_tm1\n      trig_max-normal_tm1\n      gagne_sum_tm1\n      gagne_sum_t\n    \n  \n  \n    \n      0\n      1.987430\n      0\n      1200.0\n      0.0\n      NaN\n      5.4\n      NaN\n      1.110000\n      194.0\n      white\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      7.677934\n      0\n      2600.0\n      0.0\n      119.0\n      5.5\n      40.4\n      0.860000\n      93.0\n      white\n      ...\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      4\n      3\n    \n    \n      2\n      0.407678\n      0\n      500.0\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      white\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      0.798369\n      0\n      1300.0\n      0.0\n      117.0\n      NaN\n      NaN\n      NaN\n      NaN\n      white\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      17.513165\n      0\n      1100.0\n      0.0\n      116.0\n      NaN\n      34.1\n      1.303333\n      53.0\n      white\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      48779\n      0.611517\n      0\n      800.0\n      0.0\n      NaN\n      NaN\n      NaN\n      1.090000\n      148.0\n      white\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      48780\n      2.615933\n      0\n      2200.0\n      0.0\n      112.0\n      NaN\n      41.4\n      0.810000\n      172.0\n      white\n      ...\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      1\n      1\n    \n    \n      48781\n      1.358926\n      0\n      800.0\n      0.0\n      105.0\n      NaN\n      NaN\n      NaN\n      NaN\n      white\n      ...\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      1\n      0\n    \n    \n      48782\n      10.990318\n      0\n      1300.0\n      0.0\n      132.0\n      NaN\n      NaN\n      NaN\n      NaN\n      white\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      3\n      3\n    \n    \n      48783\n      1.681671\n      0\n      4400.0\n      0.0\n      115.0\n      5.6\n      36.6\n      0.940000\n      NaN\n      white\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n48784 rows × 160 columns\n\n\n\n\nhealth_risk.shape # 48,000 entries with 160 different variables\n\n(48784, 160)\n\n\n\nhealth_risk.describe() # summary statistics \n\n\n\n\n\n  \n    \n      \n      risk_score_t\n      program_enrolled_t\n      cost_t\n      cost_avoidable_t\n      bps_mean_t\n      ghba1c_mean_t\n      hct_mean_t\n      cre_mean_t\n      ldl_mean_t\n      dem_female\n      ...\n      trig_min-high_tm1\n      trig_min-normal_tm1\n      trig_mean-low_tm1\n      trig_mean-high_tm1\n      trig_mean-normal_tm1\n      trig_max-low_tm1\n      trig_max-high_tm1\n      trig_max-normal_tm1\n      gagne_sum_tm1\n      gagne_sum_t\n    \n  \n  \n    \n      count\n      48784.000000\n      48784.000000\n      48784.000000\n      48784.000000\n      38116.000000\n      13252.000000\n      21268.000000\n      23971.000000\n      19456.000000\n      48784.000000\n      ...\n      48784.000000\n      48784.000000\n      48784.000000\n      48784.000000\n      48784.000000\n      48784.000000\n      48784.000000\n      48784.000000\n      48784.000000\n      48784.000000\n    \n    \n      mean\n      4.393692\n      0.009265\n      7659.716300\n      2434.722450\n      127.333272\n      5.959331\n      40.428418\n      0.957342\n      103.820203\n      0.630596\n      ...\n      0.090091\n      0.261807\n      0.028882\n      0.097245\n      0.256088\n      0.027939\n      0.107228\n      0.251455\n      1.443137\n      1.354481\n    \n    \n      std\n      5.519582\n      0.095811\n      17989.921192\n      12058.341779\n      16.337397\n      1.000817\n      4.231178\n      0.565138\n      32.944924\n      0.482648\n      ...\n      0.286315\n      0.439623\n      0.167478\n      0.296294\n      0.436476\n      0.164801\n      0.309406\n      0.433854\n      2.048888\n      1.942488\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.000000\n      20.811111\n      0.236667\n      3.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      1.443859\n      0.000000\n      1200.000000\n      0.000000\n      118.000000\n      5.400000\n      38.100000\n      0.750000\n      80.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      50%\n      2.887719\n      0.000000\n      2800.000000\n      0.000000\n      127.000000\n      5.700000\n      40.700000\n      0.875000\n      101.000000\n      1.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      1.000000\n      1.000000\n    \n    \n      75%\n      5.350773\n      0.000000\n      6600.000000\n      100.000000\n      136.000000\n      6.100000\n      43.121970\n      1.035000\n      124.000000\n      1.000000\n      ...\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n      2.000000\n      2.000000\n    \n    \n      max\n      100.000000\n      1.000000\n      550500.000000\n      642700.000000\n      1323.000000\n      14.733333\n      70.280000\n      22.206667\n      387.000000\n      1.000000\n      ...\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      18.000000\n      17.000000\n    \n  \n\n8 rows × 159 columns\n\n\n\n\n# Take a close look at the average chronic illnesses per race \n\nsns.barplot(x = 'race', y = 'gagne_sum_tm1', data = health_risk)\n\nplt.xlabel(\"Race\")\nplt.ylabel(\"Average Chronic Illnesses\")\nplt.show()\n\n\n\n\n\n# Even though the proportion of Black patients in this data set is less than White patients, we can see that there is a \n# higher proportion of White patients having higher costs than Black patients \n\nsns.histplot(x = 'cost_t', data = health_risk, bins = 10, hue = 'race', binwidth = 2500)\nplt.xlim(0, 40000)\nplt.show()\n\n\n\n\n\n# Taking a closer look at different variables and seeing their correlations \n\ncols = ['risk_score_t', 'gagne_sum_t','cost_t']\n\nnew_health = health_risk[cols]\n\ncorr_matrix = new_health.corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n\nplt.show()\n\n\n\n\n\n# Taking a closer look at different variables and seeing their correlations \n\ncols = ['risk_score_t', 'dem_female','alcohol_elixhauser_tm1']\n\nnew_health = health_risk[cols]\n\ncorr_matrix = new_health.corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n\nplt.show()\n\n\n\n\n\n\n\n\nIn my analysis, I will perform a linear regression to determine the variables that not only contribute most to risk score\n\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn import linear_model\nfrom sklearn import model_selection\n\n\n# droppiing NaN values from our data\nhealth_risk = health_risk.dropna()\nhealth_risk.shape\n\n(6911, 160)\n\n\n\n# Changing the race variable into a binary integer rather than a string to apply to our model\nle = preprocessing.LabelEncoder()\nhealth_risk['race'] = le.fit_transform(health_risk['race'])\n\n/tmp/ipykernel_529/2640398921.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  health_risk['race'] = le.fit_transform(health_risk['race'])\n\n\n\n# creating independent and dependent variables\n\ny = health_risk['risk_score_t']\nX = health_risk.drop('risk_score_t', axis = 1)\n\n\n# splitting data into training and testing sets \n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2, random_state = 2021) # hold out 20% of data\n\n\n# fit data to linear regression model\nLR = linear_model.LinearRegression()\n\nLR.fit(X_train, y_train)\nprint(LR.score(X_train, y_train), LR.score(X_test, y_test))\n\n0.6561374047918115 0.558242044005459\n\n\n\n# will use function later to apply different combinations of variables to determine which variables give the best \n# score to predict risk score \n\ndef check_column_score(cols):\n\n    print(\"training with columns \" + str(cols))\n\n    LR = LinearRegression()\n    return cross_val_score(LR, X_train[cols], y_train, cv = 5).mean()  \n\nOnce I determine the variables that contribute most to the risk score, I would like to make 2 different visualizations with 2 different variables in relation to the risk score. Also, I would like to plot the coefficients that contribute most to the model. Finally, for my last plot, I will show the different cross validation scores I got with the different variable combinations to show which score got the best validation score."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "brunoblog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst File\n\n\n\n\n\nTesting Quarto\n\n\n\n\n\n\nMar 6, 2023\n\n\nBen Winjum\n\n\n\n\n\n\nNo matching items"
  }
]