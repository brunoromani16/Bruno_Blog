[
  {
    "objectID": "posts/Final_Project_Notebook.html",
    "href": "posts/Final_Project_Notebook.html",
    "title": "brunoblog",
    "section": "",
    "text": "In this project, I will be taking a closer look at the topic of algorithmic bias. In a world where artificial intelligence technology continues to grow exponentially with time, addressing the concern of algorithmic bias may be informational to several minority cultures. To begin with, algorithmic bias is defined to be errors or unfair choices made by machine learning algorithms. These biases arise due to a variety of reasons, some include incorrect choice of predictor variables and biased training data.\nThe article “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations” released by Science examines how an algorithm used in healthcare can perpetuate racial bias. The article focuses on an algorithm that is used to identify patients who may benefit from additional healthcare services or lower costs. The author dives deep into one specific algorithm used in the industry, which he found in his study to exhibit racial bias. It presented racial bias by assigning a higher risk score to White patients over Black patients, even though they had similar scores throughout the different variables the algorithm would use to determine health risk score. A description of the dataset and how it was obtained can be found under this link. The model is trained on 160 different independent variables, with the risk score being the target variable.\nSome of the reasons as to why I chose this dataset to analyze include having a deeper understanding of algorithms that are racially biased, and determining if this algorithm in specific is racially biased. As a first generation student and person of color, taking a closer look at the data and how this algorithm works will help me understand how algorithms can be not only racially biased, but could lead inaccurate decisions and outcomes that may lead to people of color have immense disadvantages in several aspects of life, not only the health care system.\nTherefore, with this data set, I will take a high level overview of analysis to explore how risk scores are being assigned to patients. Specifically, I will try to answer the question: What factors could make this algorithm racially biased? How is it affecting patients of color? Does this have a long term effect on patients of color?\nI will try to answer this question beginning with an exploratory data analysis of the data set. After getting more insight into the data and determining which variables can be useful in predicting risk score for a patient, I will try to apply a machine learning model (linear regression) which will show me how well it performs with the variables selected and the weight of each variable on predicting the risk score of a patient. Also, they measure risk score through the total of medical expenses variable (cost_t). Therefore, the algorithm is trained with cost_t as the dependent or target variable.\n\n\n\n\n\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n\nhealth_risk = pd.read_csv('data_new.csv')\n\n\nhealth_risk.shape\n\n(48784, 160)\n\n\nSince we have a great amount of predictor variables that are trained in this machine learning algorithm, let us take a closer look at some of the general variables in this dataset to determine which variables could greatly affect a patient’s risk score. Variables such as:\n\nCOST_T: The patient’s total medical costs.\nRACE: The patient’s self-reported race. Data only includes Black/White patients\nGAGNE_SUM_T: The total number of chronic illnesses presented by the patient within the study period.\nDEM_FEMALE: This is an indicator if the patient is a female or not. 1 for female, 0 for male\nRISK_SCORE_T: This is the score given to a patient based on the algorithm.\n\nFor more information on different variables, a data dictionary is available provided in the link from earlier.\n\ngeneral_pred = ['risk_score_t', 'cost_t', 'race', 'gagne_sum_t', 'dem_female']\nhealth_general = health_risk[general_pred]\n\n\nhealth_general.shape\n\n(48784, 5)\n\n\n\nhealth_general = health_general.dropna()\nhealth_general.shape\n\n(48784, 5)\n\n\nThere are 48,784 patients represented as rows in the data, and 5 pieces of information about each patient represented as columns. We saw NaN values in some of the columns earlier when we first imported the data, but after passing the dropna function to our dataframe, we can see that there were no NaN values in our subset data frame. Let us do some exploratory data analysis now, specifically the correlation between the variables in a heat map.\n\n# Change race column to binary or encode it from string to integers (1 White, 0 Black)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nhealth_general['race'] = le.fit_transform(health_general['race'])\n\n\n# correlation matrix heat map\n\nplt.figure(figsize=(10,10))\nsns.heatmap(health_general.corr(), cbar=True, annot=True, cmap='Blues')\nplt.show()\n\n\n\n\nThe plot above is a heatmap which shows the correlations between each variable in our general subset data. Correlation can tell us the strength and relationship between each variable, and we want to see which variables have moderately strong correlations between each other. This is to take a closer look at each of those variables and see how they could affect the prediction of risk score for a patient.\nFrom the heatmap, we will consider moderately strong correlation coefficients to have a correlation value of 0.35 or greater. We can see that our predictor (risk_score_t) has moderately strong correlations with number of chronic illnesses (gagne_sum_t) and patient’s total medical cost (cost_t). Also, we can see that the number of chronic illnesses has a moderately strong correlation with total medical cost for a patient. Let us take a closer look at these variables and see how they can affect the risk score prediction.\n\n\nsns.scatterplot(data = health_general, x = 'cost_t', y = 'risk_score_t', hue = 'race')\nplt.ylabel('Risk Score')\nplt.xlabel('Cost')\nplt.title('Risk Score vs. Cost')\nplt.legend(labels=['White', 'Black'])\nplt.show()\n\n\n\n\nBased on the scatter plot of risk score vs. cost, we can see that patients’ risk score tends to, on average, go up as total cost of medical expenses by patients goes up. We can differentiate how race may be a variable to consider as well when evaluating risk score with cost, but let us take a better look at this difference with the next plot below.\n\n# first we have to change race back to a categorical variable with categories \"Black\" and \"White\"\n\nhealth_general['race'] = [int(i) for i in health_general['race']]\nmapping = {1 : 'white' , 0 : 'black'}\nhealth_general['race'] = health_general['race'].map(mapping)\nhealth_general['race']\n\nsns.histplot(data = health_general, x = 'cost_t', hue = 'race')\nplt.xlim(0, 25000)\nplt.ylim(0, 3000)\nplt.show()\n\n# value counts for each race\nhealth_general['race'].value_counts()\n\n\n\n\nwhite    43202\nblack     5582\nName: race, dtype: int64\n\n\nEven though the proportion of Black patients in this data set is less than White patients, we can see that there is a higher proportion of White patients having higher costs than Black patients. For example, since patients of White race seem to have higher total costs for overall medical expenses, what if these expenses are to be aligned with non-critical expenses such as plastic surgery? Race shows a sign that it may be a factor in predicting risk score.\nLet us closely investigate race now with the number of illnesses per patient. Below I will show a boxplot to show the differences between both races.\n\ngrouped_race_illness = health_general.groupby(['race'])['gagne_sum_t'].mean()\n\nsns.boxplot(data = health_general, y = 'race', x = 'gagne_sum_t', hue = 'race')\nplt.xlabel('Number of Chronic Illnesses')\nplt.ylabel('Race')\nplt.title('Number of Chronic Illnesses per race')\n\ngrouped_race_illness\n\nrace\nblack    2.055536\nwhite    1.263900\nName: gagne_sum_t, dtype: float64\n\n\n\n\n\nBased on the boxplot above, we can see that there is a difference in the average number of chronic illnesses of patients that are Black and White. The average number of chronic illnesses for Black patients is higher than of White patients. Also, we can see that the Black patient’s 75th-100th percent quartile range is larger than the White patient’s 75th-100th percent quartile in the number of chronic illnesses. This shows another sign as to how these two variables (race and number of chronic illnesses) may affect the risk score of a patient. Also, from the point made earlier with White patients having higher costs than Black patients, how could they have higher costs if on average Black patients have a higher average number of chronic illnesses? We will investigate this question once we have all of our variables selected for our model to fit.\n\nsns.barplot(data = health_general, y = 'cost_t', x = 'gagne_sum_t', hue = 'race', errorbar = None)\nplt.ylabel('Cost')\nplt.xlabel('Number of Chronic Illnesses')\nplt.title('Cost vs. Number of Chronic Illnesses')\nplt.legend(labels=['White', 'Black'])\nplt.show()\n\n\n\n\nTaking a closer look at our last two variables that have a moderately strong correlation (Cost and number of chronic illnesses) we can see that White patients on average have higher medical costs than Black patients up to a total of nine chronic illnesses. However, we see a change past 9 chronic illnesses where we see the total cost of medical expenses for Black patients is greater than the total cost of White patients. However, there is a lot of variation between both races and the confidence interval after nine chronic illnesses has a much wider range of costs (on some of the bins, White patients have higher costs than Black patients). We removed this from the plot because it does not seem fit to show the difference between cost prices and number of chronic illnesses in the first 9 points of number of chronic illnesses.\nAfter performing a general exploratory data analysis, I will move on to select the features for my model. In respect to my model, I will perform linear regression on the variables I select. Specifically, I will have the total cost of medical expenses as my dependent variable (target variable), and my independent variables will be selected in my next section that will discuss feature selection.\n\n\n\n\n\nBefore I begin to model, I will perform a systematic feature selection to select the variables for my model. Since there are 160 independent variables in my model, finding the best variables for my model through an automated function will make this process extensive. Therefore, I will select variables that are the most general variables to the algorithm’s model, and will move forward to apply a systematic feature selection in which I will choose the best variables that give me the best average cross validation score with our model. We would like to select the model with the highest cross validation score because it will give me a rough idea as to which variables the model best performs on with unseen data from our training dataset.\nHaving mentioned the training dataset, I will split my data into training and test dataset. This training dataset will train my model with this data, and we will try to predict scores with the test set to determine how well our model performed. Also, as mentioned in the introduction, we will be using total medical costs (cost_t) as our dependent variable and the rest will be our potential independent variables for our model.\n\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn import linear_model\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nThe variables I will be selecting to model with are the general variables from my exploratory data analysis (‘race’, ‘gagne_sum_t’, ‘dem_female’), and I selected three other variables that may have a potential in affecting risk score (‘cost_avoidable_t’, ‘gagne_sum_tm1’, ‘program_enrolled_t’). Below I will provide a definition for each:\n\nCOST_AVOIDABLE_T: Total cost of avoidable medical expenditures.\nGAGNE_SUM_TM1: Total number of active illnesses.\nPROGRAM_ENROLLED_T: Indicator for patients enrolled in the program or not.\n\nTo begin with, I will create a cleaning function that will clean my dataset with the given columns I pass it to the function. This will clean my data only on the columns I want to subset from the dataset.\n\n# Let us select the general columns that we will use and apply a cleaning function that will clean our data\n# we also want to encode variables that are categories into integers \n\nhealth_risk = pd.read_csv('data_new.csv')\n\ncols = ['cost_t','gagne_sum_t', 'dem_female', 'race',\n        'cost_avoidable_t', 'gagne_sum_tm1', 'program_enrolled_t']\n\ndef clean_data(df, cols):\n    df = df[cols]\n    df = df.dropna()\n    df['race'] = le.fit_transform(df['race'])\n    return df\n\nhealth_risk = clean_data(health_risk, cols)\nhealth_risk.head()\n\n\n\n\n\n  \n    \n      \n      cost_t\n      gagne_sum_t\n      dem_female\n      race\n      cost_avoidable_t\n      gagne_sum_tm1\n      program_enrolled_t\n    \n  \n  \n    \n      0\n      1200.0\n      0\n      0\n      1\n      0.0\n      0\n      0\n    \n    \n      1\n      2600.0\n      3\n      1\n      1\n      0.0\n      4\n      0\n    \n    \n      2\n      500.0\n      0\n      1\n      1\n      0.0\n      0\n      0\n    \n    \n      3\n      1300.0\n      0\n      1\n      1\n      0.0\n      0\n      0\n    \n    \n      4\n      1100.0\n      1\n      1\n      1\n      0.0\n      1\n      0\n    \n  \n\n\n\n\n\nhealth_risk.shape\n\n(48784, 7)\n\n\nAfter subsetting and cleaning my data, we can now move forward with splitting my data into train/test sets and doing a systematic feature selection. Ideally, we would be keeping our variables from the exploratory data analysis to fit our model, and will select the other three variables if they increase our overall cross validation scores. We will create the different combinations for the columns we would like to subset from our training data and model with. Since I will be keeping the variables from my exploratory data analysis in the list of combinations, this leaves us with a total of 8 list combinations to see which subset of variables will give us the best mean cross validation score.\n\n# setting seed for reporducibility and apply train/test function to split data \n\nnp.random.seed(1234)\nhealth_train, health_test = train_test_split(health_risk, test_size = 20)\n\n\n# function to calcualte mean cross validation score from \ndef cv_scores(cols):\n\n    print(\"training with columns \" + str(cols))\n    model = LinearRegression()\n    return cross_val_score(model, health_train[cols], health_train['cost_t'], cv = 5).mean() \n\n\n# column combinations to see which variables give the greatest mean cross validation score when modeling training data\n\ncolumn_comb = [['gagne_sum_t', 'dem_female', 'race', 'cost_avoidable_t', 'program_enrolled_t', 'gagne_sum_tm1'],\n              ['gagne_sum_t', 'dem_female', 'race', 'gagne_sum_tm1', 'program_enrolled_t'],\n              ['gagne_sum_t', 'dem_female', 'race', 'cost_avoidable_t', 'program_enrolled_t'],\n              ['gagne_sum_t', 'dem_female', 'race', 'cost_avoidable_t', 'gagne_sum_tm1'],\n              ['gagne_sum_t', 'dem_female', 'race', 'cost_avoidable_t'],\n              ['gagne_sum_t', 'dem_female', 'race', 'gagne_sum_tm1'],\n              ['gagne_sum_t', 'dem_female', 'race', 'program_enrolled_t'],\n              ['gagne_sum_t', 'dem_female', 'race']]\n\nfor i in column_comb:\n    score = cv_scores(i)\n    print(\"CV score is \" + str(np.round(score, 3)))\n    \n\ntraining with columns ['gagne_sum_t', 'dem_female', 'race', 'cost_avoidable_t', 'program_enrolled_t', 'gagne_sum_tm1']\nCV score is 0.706\ntraining with columns ['gagne_sum_t', 'dem_female', 'race', 'gagne_sum_tm1', 'program_enrolled_t']\nCV score is 0.129\ntraining with columns ['gagne_sum_t', 'dem_female', 'race', 'cost_avoidable_t', 'program_enrolled_t']\nCV score is 0.705\ntraining with columns ['gagne_sum_t', 'dem_female', 'race', 'cost_avoidable_t', 'gagne_sum_tm1']\nCV score is 0.706\ntraining with columns ['gagne_sum_t', 'dem_female', 'race', 'cost_avoidable_t']\nCV score is 0.705\ntraining with columns ['gagne_sum_t', 'dem_female', 'race', 'gagne_sum_tm1']\nCV score is 0.126\ntraining with columns ['gagne_sum_t', 'dem_female', 'race', 'program_enrolled_t']\nCV score is 0.129\ntraining with columns ['gagne_sum_t', 'dem_female', 'race']\nCV score is 0.126\n\n\nBased on the scores we received above, we can see that our model performed best with the columns - ‘gagne_sum_t’, ‘dem_female’, ‘race’, ‘cost_avoidable_t’, ‘gagne_sum_tm1’. The model does not differ much as compared to two other lists of variable combinations, but we will pick this set because it has the least variables. We do this to make our model less complex. We will move forward with fitting our model with these variables since they score best in our systematic feature selection.\n\n\n\nLet us now create our X and y for train/test to be able to fit our model with the linear regression function.\n\n# creating independent and dependent variables\ncols = ['cost_t', 'gagne_sum_t', 'dem_female', 'race', 'cost_avoidable_t', 'gagne_sum_tm1']\nhealth_train = health_train[cols]\nhealth_test = health_test[cols]\n\nX_train = health_train.drop('cost_t', axis = 1)\ny_train = health_train['cost_t']\n\nX_test = health_test.drop('cost_t', axis = 1)\ny_test = health_test['cost_t']\n\nWe will now fit our training data to our model and display the coefficient of determination scores.\n\n# fit data to linear regression model\nLR = linear_model.LinearRegression()\n\nLR.fit(X_train, y_train)\nprint(LR.score(X_train, y_train), LR.score(X_test, y_test))\n\n0.7079527773610754 0.9266021681495551\n\n\nBased on the results, our model did pretty well on the testing data set. Even though we did not get as high of a score in our training set, this shows that our model is not overfitting the data. We can see this by getting a lower training score than our test score. This indicated that our model is able to generalize well to new and unseen data (testing data).\n\n\n\n\n\nWe will now take a closer look at the results of our model, specifically our coefficeints. We want to see which variables contribute most to our model in predicting risk score.\n\n# get coefficients from fit model \n\nLR.fit(X_train, y_train).coef_\n\narray([ 587.45140809, 1002.14821293,  509.748747  ,    1.19313914,\n        514.53466781])\n\n\n\n# create table to clearly show results \n\nresults = {'Predictors' : ['Chronic Illnesses', 'Gender', 'Race', 'Avoided Costs', 'Active Chronic Illnesses'],\n          'Coefficients' : [ 587.45140809, 1002.14821293, 509.748747, 1.19313914, 514.53466781]}\nresults_df = pd.DataFrame(results)\nresults_df\n\n\n\n\n\n  \n    \n      \n      Predictors\n      Coefficients\n    \n  \n  \n    \n      0\n      Chronic Illnesses\n      587.451408\n    \n    \n      1\n      Gender\n      1002.148213\n    \n    \n      2\n      Race\n      509.748747\n    \n    \n      3\n      Avoided Costs\n      1.193139\n    \n    \n      4\n      Active Chronic Illnesses\n      514.534668\n    \n  \n\n\n\n\n\nplt.figure(figsize = (15,5))\nsns.barplot(data = results_df, x = 'Predictors', y = 'Coefficients')\nplt.show()\n\n\n\n\nIf we focus on the categorical variables, such as gender or race, we can see that they have a great deal in determining overall costs - which then correlates with overall risk score for a patient. For example, taking a look at race, we know that in our dataset 0 denotes Black and 1 White. This shows that if the patient is White, the model will automatically increase the overall cost of a patient by roughly 510 dollars (obtained this from the coefficient and multiplying it by 1). This explains why there may be an issue between Black and White patients getting different risk scores, even though they might have similar risk symptoms. Also, we can see the same for gender. If a patient is a female (denoted as 1 in our dataset) we see that they would automatically roughly get an additional 1002 dollars added to their total cost. This shows that females may, on average, get higher risk scores than males.\n\nhealth_risk = pd.read_csv('data_new.csv')\n\nf = sns.relplot(data = health_risk, x='gagne_sum_t', y='cost_t', col='dem_female', hue = 'race', \n                hue_order = ['black', 'white'], kind='line')\n\nf.set_axis_labels( \"Number of chronic illnesses\", \"Risk Score\")\n\nf.facet_axis(0, 0).set_title(\"Male \")\nf.facet_axis(0, 1).set_title(\"Female\")\nf.legend.set_title(\"Race\")\nf.legend.texts[0].set_text(\"Black\")\nf.legend.texts[1].set_text(\"White\")\n\nplt.show()\n\n\n\n\nSince gender and race are both variables that have a strong influence on cost/risk score, let us take a closer look at them and how they affect the number of chronic illnesses. We can see here, both races seem to have equal differences in risk scores. However, if we take a closer look at the first 10 chronic illnesses, we can see that White male patients seem to have a higher risk score over Black male patients. For females, we can say the same for the first 8 chronic diseases. However, there is much variation in this plot and cannot fully see a difference between both races.\n\n\n\nBased on our results, we can see that the gender and race coefficients have a significant factor in predicting cost. Since the risk score a patient receives is a function of the model’s prediction of the total medical costs which will be incurred by that individual, we can see that patients of Black race will receive a lower total for medical costs, which will ultimately lead to a lower risk score. This shows a huge disparity between White and Black patients, in which, if you are White, you can get a $579.90 expected extra medical cost, which can lead to a higher overall risk score. This leads to a systematic bias in the algorithm.\nTherefore, if this model is still used to calculate risk scores for patients, Black patients are being held back to getting much better benefits since they have lower risk scores. This matter is not only affecting Black patients’ long term health, but also is a systemic issue that will continue to persist until it is solved. This algorithm will continue to have a long term effect on patients of color, and this is only one algorithm analyzed out of the many algorithms used in place in the healthcare industry.\nArtificial intelligence will continue to exponentially grow with time, and we will have to adapt to it as well. However, algorithms that have bias, such as racial bias, must be trained with the proper data. The people behind building these models must now take a closer look at how these models are built and with what kind of data. For example, obtaining data from a representative and diverse population may be one way to solve algorithmic bias. Another way to help solve this issue is for the creators of the models to be explicit and transparent with their methods. With clear documentation on the algorithm, other people may be able to give another eye to the model and provide insights into the model if it has bias or not. Algorithmic bias is here to stay for the near future when building models, but we can minimize it by being aware of these models and understanding their functionality."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "brunoblog",
    "section": "",
    "text": "Consequences of Algorithmic Bias\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]